# SimpleLLM
A simple LLM app using Ollama e Streamlit with Docker

docker compose up -d

docker exec -it ollama ollama pull llama3

curl http://localhost:11434